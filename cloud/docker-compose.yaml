# This file defines the services involved in hosting Transitive. It is shared
# by all four modes defined in
# https://github.com/transitiverobotics/transitive/wiki#deployment-modes.
# The two rows of that table, dev and prod, are distinguished via compose
# profiles of the same names.

version: "3.9"

# --------------------------------------------------------------------------
# "Extensions": service definitions shared between dev and prod

x-cloud: &shared_cloud
  container_name: cloud
  image: transitiverobotics/cloud:latest
  tty: true # to get colored logging output
  restart: always
  depends_on:
    mongodb:
      condition: service_started
    ensure_certs:
      condition: service_completed_successfully
  env_file:
    - .env
  ports:
    - 9000:9000
  networks:
    - default
    - caps
    - shared
  logging:
    driver: local
  deploy:
    resources:
      limits:
        cpus: '0.9'
        memory: 800M

x-mosquitto: &shared_mosquitto
  container_name: mosquitto
  image: transitiverobotics/mosquitto:latest
  restart: always
  depends_on:
    ensure_certs:
      condition: service_completed_successfully
  env_file:
    - .env
  volumes:
    - ${TR_VAR_DIR:-.}/certs:/mosquitto/certs
    - ${TR_VAR_DIR:-.}/mqtt_persistence:/persistence
  ports:
    - 8883:8883
    - 9001:9001
  networks:
    - default
    - caps
  cap_add:
    - NET_ADMIN
    - NET_RAW
  logging:
    driver: local
  deploy:
    resources:
      limits:
        cpus: '0.9'
        memory: 850M

x-registry: &shared_registry
  container_name: registry
  image: transitiverobotics/registry:latest
  restart: always
  depends_on:
    - mongodb
  env_file:
    - .env
  ports:
    - 6000:6000
  networks:
    - default
    - shared
    - caps
  logging:
    driver: local
  deploy:
    resources:
      limits:
        cpus: '0.7'
        memory: 500M

# --------------------------------------------------------------------------

networks:
  caps:
    attachable: true
  shared:
    attachable: true

services:
  # a minimal container that ensures certificates are present in the shared,
  # mounted volume
  ensure_certs:
    image: transitiverobotics/ensure_certs:latest
    volumes:
      - ${TR_VAR_DIR:-.}/certs:/generated

  # MongoDB with replset, required to watch for changes
  mongodb:
    image: transitiverobotics/mongo:latest
    restart: always
    command: --replSet rs0
    extra_hosts:
      - mongodb:127.0.0.1 # required for init script to set up replset
    volumes:
      - ${TR_VAR_DIR:-.}/db:/data/db
    networks:
      - default
      - caps # TODO: This means we'll need to enable authentication on the DB
      - shared
    logging:
      driver: local
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 600M

  mosquitto_prod:
    <<: *shared_mosquitto
    profiles:
      - prod

  mosquitto_dev:
    <<: *shared_mosquitto
    profiles:
      - dev
    volumes:
      - ${TR_VAR_DIR:-.}/certs:/mosquitto/certs
      - ${TR_VAR_DIR:-.}/mqtt_persistence:/persistence

  cloud_prod:
    <<: *shared_cloud
    profiles:
      - prod
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${TR_VAR_DIR:-.}/certs:/etc/mosquitto/certs

  cloud_dev:
    <<: *shared_cloud
    profiles:
      - dev
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${TR_VAR_DIR:-.}/certs:/etc/mosquitto/certs

  proxy:
    image: transitiverobotics/proxy:latest
    restart: always
    volumes:
      - ${TR_VAR_DIR:-.}/greenlock.d:/app/greenlock.d
    env_file:
      - .env
    ports:
      - 80:80
      - 443:443
    networks:
      - default
      - shared
    logging:
      driver: local
    deploy:
      resources:
        limits:
          cpus: '0.9'
          memory: 300M

  registry_prod:
    <<: *shared_registry
    profiles:
      - prod

  registry_dev:
    <<: *shared_registry
    profiles:
      - dev
    volumes:
      # In dev we allow serving local dev files for capability bundles. This
      # assumes a specific directory layout.
      - /tmp/caps:/app/caps:ro

  # In Dev: run a small mDNS service to allow local testing with subdomains
  mdns:
    image: transitiverobotics/mdns:latest
    restart: always
    volumes:
      - /etc:/host_etc
    env_file:
      - .env
    network_mode: host
    logging:
      driver: local
    profiles:
      - dev

  # hyperdx:
  #   image: docker.hyperdx.io/hyperdx/hyperdx-all-in-one
  #   volumes:
  #     - ${TR_VAR_DIR:-.}/hyperdx/db:/data/db"
  #     - ${TR_VAR_DIR:-.}/hyperdx/ch_data:/var/lib/clickhouse"
  #     - ${TR_VAR_DIR:-.}/hyperdx/ch_logs:/var/log/clickhouse-server"
  #   env_file:
  #     - .env

  # -----------------------------------------------------------------------
  # HyperDX

  clickhouse:
    image: clickhouse/clickhouse-server:24-alpine
    env_file:
      - .env
    environment:
      # default settings
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    volumes:
      # - ./docker/clickhouse/local/config.xml:/etc/clickhouse-server/config.xml
      # - ./docker/clickhouse/local/users.xml:/etc/clickhouse-server/users.xml
      - ${TR_VAR_DIR:-.}/hyperdx/ch_data:/var/lib/clickhouse"
      - ${TR_VAR_DIR:-.}/hyperdx/ch_logs:/var/log/clickhouse-server"
    restart: on-failure
    networks:
      - default

  otel-collector:
    image: docker.hyperdx.io/hyperdx/hyperdx-otel-collector:2
    env_file:
      - .env
    environment:
      CLICKHOUSE_ENDPOINT: 'tcp://clickhouse:9000?dial_timeout=10s'
      HYPERDX_OTEL_EXPORTER_CLICKHOUSE_DATABASE: default
      HYPERDX_LOG_LEVEL: ${HYPERDX_LOG_LEVEL:-default}
      # OPAMP_SERVER_URL: 'http://hyperdx:${HYPERDX_OPAMP_PORT:-4320}'
      OPAMP_SERVER_URL: 'http://hyperdx:4320'
    # ports:
    #   - '13133:13133' # health_check extension
    #   - '24225:24225' # fluentd receiver
    #   - '4317:4317' # OTLP gRPC receiver
    #   - '4318:4318' # OTLP http receiver
    #   - '8888:8888' # metrics extension
    restart: always
    networks:
      - default
    depends_on:
      - clickhouse

  hyperdx:
    image: docker.hyperdx.io/hyperdx/hyperdx:2
    # ports:
    #   - ${HYPERDX_API_PORT:-8000}:${HYPERDX_API_PORT:-8000}
    #   - ${HYPERDX_APP_PORT:-8080}:${HYPERDX_APP_PORT:-8080}
    env_file:
      - .env
    environment:
      FRONTEND_URL: ${HYPERDX_APP_URL:-http://localhost}:${HYPERDX_APP_PORT:-8000}
      # HYPERDX_API_KEY: ${HYPERDX_API_KEY:-12345678901234567890}
      # HYPERDX_API_PORT: ${HYPERDX_API_PORT:-8000}
      # HYPERDX_APP_PORT: ${HYPERDX_APP_PORT:-8080}
      # HYPERDX_APP_URL: ${HYPERDX_APP_URL:-http://localhost}
      # HYPERDX_LOG_LEVEL: ${HYPERDX_LOG_LEVEL:-debug}
      # MINER_API_URL: 'http://miner:5123'
      MONGO_URI: 'mongodb://mongodb:27017/hyperdx'
      NEXT_PUBLIC_SERVER_URL: http://127.0.0.1:${HYPERDX_API_PORT:-8000}
      # OPAMP_PORT: ${HYPERDX_OPAMP_PORT:-4320}
      OPAMP_PORT: 4320
      OTEL_SERVICE_NAME: 'hdx-oss-api'
      USAGE_STATS_ENABLED: ${USAGE_STATS_ENABLED:-true}
      DEFAULT_CONNECTIONS:
        '[{"name":"Local
        ClickHouse","host":"http://clickhouse:8123","username":"default","password":""}]'
      DEFAULT_SOURCES:
        '[{"from":{"databaseName":"default","tableName":"otel_logs"},"kind":"log","timestampValueExpression":"TimestampTime","name":"Logs","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"Body","serviceNameExpression":"ServiceName","bodyExpression":"Body","eventAttributesExpression":"LogAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,SeverityText,Body","severityTextExpression":"SeverityText","traceIdExpression":"TraceId","spanIdExpression":"SpanId","connection":"Local
        ClickHouse","traceSourceId":"Traces","sessionSourceId":"Sessions","metricSourceId":"Metrics"},{"from":{"databaseName":"default","tableName":"otel_traces"},"kind":"trace","timestampValueExpression":"Timestamp","name":"Traces","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"SpanName","serviceNameExpression":"ServiceName","bodyExpression":"SpanName","eventAttributesExpression":"SpanAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,StatusCode,round(Duration/1e6),SpanName","traceIdExpression":"TraceId","spanIdExpression":"SpanId","durationExpression":"Duration","durationPrecision":9,"parentSpanIdExpression":"ParentSpanId","spanNameExpression":"SpanName","spanKindExpression":"SpanKind","statusCodeExpression":"StatusCode","statusMessageExpression":"StatusMessage","connection":"Local
        ClickHouse","logSourceId":"Logs","sessionSourceId":"Sessions","metricSourceId":"Metrics"},{"from":{"databaseName":"default","tableName":""},"kind":"metric","timestampValueExpression":"TimeUnix","name":"Metrics","resourceAttributesExpression":"ResourceAttributes","metricTables":{"gauge":"otel_metrics_gauge","histogram":"otel_metrics_histogram","sum":"otel_metrics_sum","_id":"682586a8b1f81924e628e808","id":"682586a8b1f81924e628e808"},"connection":"Local
        ClickHouse","logSourceId":"Logs","traceSourceId":"Traces","sessionSourceId":"Sessions"},{"from":{"databaseName":"default","tableName":"hyperdx_sessions"},"kind":"session","timestampValueExpression":"TimestampTime","name":"Sessions","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"Body","serviceNameExpression":"ServiceName","bodyExpression":"Body","eventAttributesExpression":"LogAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,SeverityText,Body","severityTextExpression":"SeverityText","traceIdExpression":"TraceId","spanIdExpression":"SpanId","connection":"Local
        ClickHouse","logSourceId":"Logs","traceSourceId":"Traces","metricSourceId":"Metrics"}]'
    networks:
      - default
    depends_on:
      - clickhouse
      - mongodb

