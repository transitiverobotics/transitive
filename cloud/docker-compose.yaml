# This file defines the services involved in hosting Transitive. It is shared
# by all four modes defined in
# https://github.com/transitiverobotics/transitive/wiki#deployment-modes.
# The two rows of that table, dev and prod, are distinguished via compose
# profiles of the same names.

# --------------------------------------------------------------------------
# "Extensions": service definitions shared between dev and prod

x-cloud: &shared_cloud
  container_name: cloud
  image: transitiverobotics/cloud:latest
  tty: true # to get colored logging output
  restart: always
  depends_on:
    mongodb:
      condition: service_started
    ensure_certs:
      condition: service_completed_successfully
  env_file:
    - .env
  ports:
    - 9000:9000
  networks:
    - default
    - caps
    - shared
  logging:
    driver: local
  deploy:
    resources:
      limits:
        cpus: '0.9'
        memory: 800M

x-mosquitto: &shared_mosquitto
  container_name: mosquitto
  image: transitiverobotics/mosquitto:latest
  restart: always
  depends_on:
    ensure_certs:
      condition: service_completed_successfully
  env_file:
    - .env
  volumes:
    - ${TR_VAR_DIR:-.}/certs:/mosquitto/certs
    - ${TR_VAR_DIR:-.}/mqtt_persistence:/persistence
  ports:
    - 8883:8883
    - 9001:9001
  networks:
    - default
    - caps
  cap_add:
    - NET_ADMIN
    - NET_RAW
  logging:
    driver: local
  deploy:
    resources:
      limits:
        cpus: '0.9'
        memory: 850M

x-registry: &shared_registry
  container_name: registry
  image: transitiverobotics/registry:latest
  restart: always
  depends_on:
    - mongodb
  env_file:
    - .env
  ports:
    - 6000:6000
  networks:
    - default
    - shared
    - caps
  logging:
    driver: local
  deploy:
    resources:
      limits:
        cpus: '0.7'
        memory: 500M

# --------------------------------------------------------------------------

networks:
  caps:
    attachable: true
  shared:
    attachable: true

services:
  # a minimal container that ensures certificates are present in the shared,
  # mounted volume
  ensure_certs:
    image: transitiverobotics/ensure_certs:latest
    volumes:
      - ${TR_VAR_DIR:-.}/certs:/generated

  # MongoDB with replset, required to watch for changes
  mongodb:
    image: transitiverobotics/mongo:latest
    restart: always
    command: --replSet rs0
    extra_hosts:
      - mongodb:127.0.0.1 # required for init script to set up replset
    volumes:
      - ${TR_VAR_DIR:-.}/db:/data/db
    networks:
      - default
      - caps # TODO: This means we'll need to enable authentication on the DB
      - shared
    logging:
      driver: local
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 600M

  mosquitto_prod:
    <<: *shared_mosquitto
    profiles:
      - prod

  mosquitto_dev:
    <<: *shared_mosquitto
    profiles:
      - dev
    volumes:
      - ${TR_VAR_DIR:-.}/certs:/mosquitto/certs
      - ${TR_VAR_DIR:-.}/mqtt_persistence:/persistence

  cloud_prod:
    <<: *shared_cloud
    profiles:
      - prod
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${TR_VAR_DIR:-.}/certs:/etc/mosquitto/certs

  cloud_dev:
    <<: *shared_cloud
    profiles:
      - dev
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${TR_VAR_DIR:-.}/certs:/etc/mosquitto/certs

  proxy:
    image: transitiverobotics/proxy:latest
    restart: always
    volumes:
      - ${TR_VAR_DIR:-.}/greenlock.d:/app/greenlock.d
    env_file:
      - .env
    ports:
      - 80:80
      - 443:443
    networks:
      - default
      - shared
    logging:
      driver: local
    deploy:
      resources:
        limits:
          cpus: '0.9'
          memory: 300M

  registry_prod:
    <<: *shared_registry
    profiles:
      - prod

  registry_dev:
    <<: *shared_registry
    profiles:
      - dev
    volumes:
      # In dev we allow serving local dev files for capability bundles. This
      # assumes a specific directory layout.
      - /tmp/caps:/app/caps:ro

  # In Dev: run a small mDNS service to allow local testing with subdomains
  mdns:
    image: transitiverobotics/mdns:latest
    restart: always
    volumes:
      - /etc:/host_etc
    env_file:
      - .env
    network_mode: host
    logging:
      driver: local
    profiles:
      - dev

  # hyperdx:
  #   image: docker.hyperdx.io/hyperdx/hyperdx-all-in-one
  #   volumes:
  #     - ${TR_VAR_DIR:-.}/hyperdx/db:/data/db"
  #     - ${TR_VAR_DIR:-.}/hyperdx/ch_data:/var/lib/clickhouse"
  #     - ${TR_VAR_DIR:-.}/hyperdx/ch_logs:/var/log/clickhouse-server"
  #   env_file:
  #     - .env

  # -----------------------------------------------------------------------
  # HyperDX

  # clickhouse:
  #   image: clickhouse/clickhouse-server:24-alpine
  #   env_file:
  #     - .env
  #   environment:
  #     # default settings
  #     CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
  #   volumes:
  #     - ${TR_VAR_DIR:-.}/hyperdx/ch_data:/var/lib/clickhouse
  #     # - ${TR_VAR_DIR:-.}/hyperdx/ch_logs:/var/log/clickhouse-server
  #   restart: always
  #   networks:
  #     - default
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         # memory: 900M

  # hyperdx:
  #   image: docker.hyperdx.io/hyperdx/hyperdx:2
  #   env_file:
  #     - .env
  #   environment:
  #     # FRONTEND_URL: ${HYPERDX_APP_URL:-http://localhost}:${HYPERDX_APP_PORT:-80}
  #     FRONTEND_URL: ${HYPERDX_APP_URL:-http://localhost}
  #     MONGO_URI: 'mongodb://mongodb:27017/hyperdx'
  #     NEXT_PUBLIC_SERVER_URL: http://127.0.0.1:${HYPERDX_API_PORT:-8000}
  #     OPAMP_PORT: 4320
  #     OTEL_SERVICE_NAME: 'hdx-oss-api'
  #     USAGE_STATS_ENABLED: ${USAGE_STATS_ENABLED:-true}
  #     DEFAULT_CONNECTIONS:
  #       '[{"name":"Local
  #       ClickHouse","host":"http://clickhouse:8123","username":"default","password":""}]'
  #     DEFAULT_SOURCES:
  #       '[{"from":{"databaseName":"default","tableName":"otel_logs"},"kind":"log","timestampValueExpression":"TimestampTime","name":"Logs","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"Body","serviceNameExpression":"ServiceName","bodyExpression":"Body","eventAttributesExpression":"LogAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,SeverityText,Body","severityTextExpression":"SeverityText","traceIdExpression":"TraceId","spanIdExpression":"SpanId","connection":"Local
  #       ClickHouse","traceSourceId":"Traces","sessionSourceId":"Sessions","metricSourceId":"Metrics"},{"from":{"databaseName":"default","tableName":"otel_traces"},"kind":"trace","timestampValueExpression":"Timestamp","name":"Traces","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"SpanName","serviceNameExpression":"ServiceName","bodyExpression":"SpanName","eventAttributesExpression":"SpanAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,StatusCode,round(Duration/1e6),SpanName","traceIdExpression":"TraceId","spanIdExpression":"SpanId","durationExpression":"Duration","durationPrecision":9,"parentSpanIdExpression":"ParentSpanId","spanNameExpression":"SpanName","spanKindExpression":"SpanKind","statusCodeExpression":"StatusCode","statusMessageExpression":"StatusMessage","connection":"Local
  #       ClickHouse","logSourceId":"Logs","sessionSourceId":"Sessions","metricSourceId":"Metrics"},{"from":{"databaseName":"default","tableName":""},"kind":"metric","timestampValueExpression":"TimeUnix","name":"Metrics","resourceAttributesExpression":"ResourceAttributes","metricTables":{"gauge":"otel_metrics_gauge","histogram":"otel_metrics_histogram","sum":"otel_metrics_sum","_id":"682586a8b1f81924e628e808","id":"682586a8b1f81924e628e808"},"connection":"Local
  #       ClickHouse","logSourceId":"Logs","traceSourceId":"Traces","sessionSourceId":"Sessions"},{"from":{"databaseName":"default","tableName":"hyperdx_sessions"},"kind":"session","timestampValueExpression":"TimestampTime","name":"Sessions","displayedTimestampValueExpression":"Timestamp","implicitColumnExpression":"Body","serviceNameExpression":"ServiceName","bodyExpression":"Body","eventAttributesExpression":"LogAttributes","resourceAttributesExpression":"ResourceAttributes","defaultTableSelectExpression":"Timestamp,ServiceName,SeverityText,Body","severityTextExpression":"SeverityText","traceIdExpression":"TraceId","spanIdExpression":"SpanId","connection":"Local
  #       ClickHouse","logSourceId":"Logs","traceSourceId":"Traces","metricSourceId":"Metrics"}]'
  #   networks:
  #     - default
  #   restart: always
  #   depends_on:
  #     - clickhouse
  #     - mongodb

  # # The OpenTelemetry collector for ClickHouse/HyperDX (ClickStack)
  # otel-collector:
  #   image: docker.hyperdx.io/hyperdx/hyperdx-otel-collector:2
  #   env_file:
  #     - .env
  #   environment:
  #     CLICKHOUSE_ENDPOINT: 'tcp://clickhouse:9000?dial_timeout=10s'
  #     HYPERDX_OTEL_EXPORTER_CLICKHOUSE_DATABASE: default
  #     OPAMP_SERVER_URL: 'http://hyperdx:4320'
  #   restart: always
  #   networks:
  #     - default
  #   depends_on:
  #     - clickhouse
  #     - hyperdx
